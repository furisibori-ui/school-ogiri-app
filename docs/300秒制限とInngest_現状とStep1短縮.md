# 300秒制限の理由・現状（Gemini 等に伝える用）と Step1 短縮案

## なぜ 300 秒に収めているのか（Inngest を入れたのに超えられない理由）

- **Inngest を入れた目的**: フロントが「学校生成」を押したあと、**長時間待たずに jobId だけ受け取り、バックグラウンドで生成を進める**ようにするため。フロントはポーリングするだけなので、**ユーザー体験**は 300 秒に縛られない。
- **しかし実行場所は Vercel のまま**: Inngest の「学校生成」関数が動くとき、**実際に API を叩いているのは Vercel 上のサーバーレス関数**です。
  - Inngest が「このジョブを実行して」と **1 回の HTTP リクエスト**で `/api/inngest` を呼ぶ。
  - その 1 回のリクエストのあいだに、**Step1（テキスト）→ Step2（画像7枚）→ Step3（校歌）** を **直列**で実行している。
  - Vercel Pro では **1 リクエストあたり maxDuration = 300 秒**が上限。
- **だから**: 「Step1 + Step2 + Step3 の合計」が 300 秒を超えると、**Vercel がそのリクエストを強制終了**し、`FUNCTION_INVOCATION_TIMEOUT` になる。Inngest 側では「時間切れで途中まで」の状態になる。
- **結論**: Inngest にしたからといって「1 本の実行」が 300 秒を超えられるわけではない。300 秒は **Vercel の 1 リクエストの上限**なので、**その 1 回の実行全体**を 300 秒以内に収める必要がある。

---

## 現状の時間配分（Gemini に伝えるときの説明）

| 項目 | 内容 |
|------|------|
| **プラットフォーム** | Vercel Pro。サーバーレス関数の maxDuration は **300 秒**が上限。 |
| **実行形態** | Inngest の 1 ジョブ = 1 回の HTTP で `/api/inngest` が呼ばれ、その中で Step1 → Step2 → Step3 を直列実行。 |
| **Step1（テキスト）** | `/api/generate-school` を内部で呼ぶ。**245 秒で打ち切り**（AI_TIMEOUT_MS）。超えると 503 でエラー返却（モックは返さない）。 |
| **Step2（画像）** | 画像 7〜8 枚を並列で生成。**実測でおおよそ 30〜40 秒**。 |
| **Step3（校歌）** | `/api/generate-audio` で Suno に submit → ポーリング。**最大 60 秒**で打ち切り。 |
| **合計** | 245 + 約36 + 60 ≒ **341 秒**になりうるため、**すでに 300 秒を超過する可能性**がある。Step1 を 240 秒に戻すか、Step3 を 50 秒にするなど、**合計が 300 を超えないよう**調整している。 |
| **テキストが間に合わないとき** | Step1 が 245 秒で終わらないと「テキスト生成が時間内に完了しませんでした」で 503。テンプレート（モック）は返さずエラーにしている。 |

**Gemini に伝える一文**:  
「Vercel の 1 リクエストが 300 秒で切れるため、Step1（テキスト）は 245 秒で打ち切っており、それを超えるとエラーにしている。Step1 を早く終わらせるか、テキスト生成を軽くして 245 秒以内に収めたい。」

---

## Step1 を早く終わらせるためにできること

### すでにやっていること

- 地域リサーチを約 70% に削減し、さらに **RESEARCH_MAX 300 字・PROPER_NOUNS_MAX 15 個**に短縮。
- プロンプト・JSON スキーマを約 70% に簡略化（文字数指定の短縮、指示の圧縮）。
- **max_tokens を 8192 → 4096 に削減**（出力トークン削減で生成時間短縮）。
- **Comet 利用時は試すモデルを 2 本に限定**（Haiku → 失敗時のみ Sonnet）。試行を増やしすぎないことで **API 消費のバーストを防止**。`COMET_CHAT_MODEL` で 1 本に固定するとさらに確実。
- タイムアウト時はモックではなく 503 エラー返却。

### さらにできること（優先度の目安）

| 優先度 | 内容 | 効果・注意 |
|--------|------|------------|
| 1 | **プロンプト・コンテキストをさらに短くする** | 実施済み: RESEARCH_MAX 300 字、PROPER_NOUNS_MAX 15。さらに短くするなら 250 字 / 12 個など。 |
| 2 | **max_tokens を下げる** | **実施済み: 4096**。さらに短くするなら 3072 など。 |
| 3 | **より速いモデルに変える** | **実施済み**: Comet 未設定時は Haiku / Gemini 1.5 Flash を優先。固定するなら `COMET_CHAT_MODEL=anthropic/claude-3-5-haiku` など。 |
| 4 | **Step1 を 2 本に分割して並列** | 「コア」（校名・概要・校長・校歌・沿革など）と「拡張」（行事・部活・施設・教員）に分け、**別々の API 呼び出し**で並列実行。それぞれ 300 秒枠なので、1 本が遅くてももう 1 本で補える。設計・マージ処理が重い。 |
| 5 | **Comet のレイテンシ確認** | 同じプロンプトで Comet 経由と直の Anthropic で比較し、Comet が遅いならプロバイダやリージョンの見直し。 |
| 6 | **キャッシュの検討** | 同じ location で再生成する場合に、テキストだけキャッシュして返す（要設計）。 |

### コードで触る場所（Step1 短縮）

- `app/api/generate-school/route.ts`
  - `RESEARCH_MAX` / `PROPER_NOUNS_MAX`（buildLocationContext）→ 現状 300 / 15
  - `AI_TIMEOUT_MS`（245_000 のままか、300 秒に余裕を持たせるなら 240 秒に戻すなど）
  - `max_tokens`（現状 **4096**）
  - `modelIds`（Comet 未設定時: Haiku / Flash を先に試し、失敗時は Sonnet）
- 環境変数 `COMET_CHAT_MODEL` で使用モデルを固定可能。例: `anthropic/claude-3-5-haiku`（速い）、`anthropic/claude-3-5-sonnet`（重いが高品質）。

---

## まとめ

- **300 秒に収めている理由**: Inngest の 1 ジョブが動く「1 本の HTTP リクエスト」が Vercel 上で 300 秒までだから。Inngest にしたのは「フロントをブロックしない」ためであり、「1 回の実行時間を伸ばす」ためではない。
- **現状**: Step1 を 245 秒で打ち切り、Step2 約 36 秒・Step3 最大 60 秒で、合計が 300 秒を超えないよう調整している。Step1 が 245 秒で終わらないと 503 でエラー。
- **Step1 を早くするには**: プロンプト・リサーチ量のさらなる削減、max_tokens の削減、速いモデルへの変更、必要に応じて Step1 の分割・並列化を検討する。
